# -*- coding: utf-8 -*-
"""Charlie_Light_Filter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18sSfZiEfV2qrmi6tFHY9c5emK81MiANT

# Imports
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, StandardScaler


# from sqlalchemy.sql.schema import ScalarElementColumnDefault
# from typing import Tuple
# from google.colab import drive

"""# DataCollector - Do Not Modify"""

class DataCollector:
    def artist_styles_one_hot(self):
        raise NotImplementedError(
            "you need to implement this, needs to be two lists, one for string one for coefficient, coefficient list is one larger to account for 'other'"
            "Coefficient is from the model after training, so to prepare training data, you can put dummy number first, then replace it later after model has been trained"
        )

    def sources_one_hot(self):
        raise NotImplementedError(
            "you need to implement this, needs to be two lists, one for string one for coefficient, coefficient list is one larger to account for 'other'"
            "Coefficient is from the model after training, so to prepare training data, you can put dummy number first, then replace it later after model has been trained"
        )

    def num_inference_steps_one_hot(self):
        raise NotImplementedError(
            "you need to implement this, needs to be two lists, one for string one for coefficient, coefficient list is one larger to account for 'other'"
            "Coefficient is from the model after training, so to prepare training data, you can put dummy number first, then replace it later after model has been trained"
        )

    def one_hot_encoding_functions(self):
        return zip(
            [self.artist_styles_one_hot(), self.sources_one_hot(), self.num_inference_steps_one_hot()],
            ['artist_style', 'source', 'num_inference_steps']
        )

    def custom_aggregation(self, prefix, data):
        result = {
            f'{prefix}_likes': np.sum((data['engagement_type'] == 'Like') & (data['engagement_value'] == 1)),
            f'{prefix}_dislikes': np.sum((data['engagement_type'] == 'Like') & (data['engagement_value'] == -1)),
            f'{prefix}_engagement_time_avg': data[data['engagement_type'] == 'MillisecondsEngagedWith']['engagement_value'].mean(),
        }
        return pd.Series(result)

    def feature_generation_user(self):
        return self.user_data.groupby('user_id').apply(lambda data: self.custom_aggregation('user', data)).reset_index()

    def feature_generation_content_one_hot_encoding(self):
        for (categories, _coefficient), col_name in self.one_hot_encoding_functions():
            self.generated_content_metadata_data[col_name] = self.generated_content_metadata_data[col_name].apply(lambda x: x if x in categories else 'other')
            encoder = OneHotEncoder(categories=[categories + ['other']], sparse_output=False)
            encoded_data = encoder.fit_transform(self.generated_content_metadata_data[[col_name]])
            encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out([col_name]))
            for col in encoded_df.columns:
              self.generated_content_metadata_data[col] = encoded_df[col]
        return self.generated_content_metadata_data

    def feature_generation_content_engagement_value(self):
        return self.engagement_data.groupby('content_id').apply(
            lambda data: self.custom_aggregation('content', data)
        ).reset_index()

    def feature_generation(self):
      self.feature_generation_user()
      self.feature_generation_content_one_hot_encoding()
      self.feature_generation_content_engagement_value()

    def get_engagement_data(self, content_ids):
      df = pd.read_csv('sample_data/engagement.csv', sep="\t")
      return df[df['content_id'].isin(content_ids)]

    def get_generated_content_metadata_data(self, content_ids):
      df = pd.read_csv('sample_data/generated_content_metadata.csv', sep="\t")
      return df[df['content_id'].isin(content_ids)]

    def get_user_data(self, user_id):
      df = pd.read_csv('sample_data/engagement.csv', sep="\t")
      return df[df['user_id'] == user_id]

    def gather_data(self, user_id, content_ids):
      self.engagement_data = self.get_engagement_data(content_ids)
      self.generated_content_metadata_data = self.get_generated_content_metadata_data(content_ids)
      self.user_data = self.get_user_data(user_id)

    def gather_training_data(self):
      self.engagement_data = pd.read_csv('sample_data/engagement.csv', sep="\t")
      self.generated_content_metadata_data = pd.read_csv('sample_data/generated_content_metadata.csv', sep="\t")
      self.user_data = pd.read_csv('sample_data/engagement.csv', sep="\t")

    def feature_eng_training(self):
      user_attr = self.feature_generation_user()
      content_engagement_features = self.feature_generation_content_engagement_value()
      generated_content_features = self.feature_generation_content_one_hot_encoding()

      interaction_pairs = self.engagement_data[
          ['user_id', 'content_id']].drop_duplicates()

      self.training_results = pd.merge(
          interaction_pairs,
          user_attr,
          on='user_id',
          how='left'
      ).fillna(0)

      content_results = pd.merge(
          generated_content_features,
          content_engagement_features,
          on='content_id',
          how='left'
      ).fillna(0)

      self.training_results = pd.merge(
          self.training_results,
          content_results,
          on='content_id',
          how='left'
      ).fillna(0)

      return self.training_results

    def feature_eng(self):
      user_attr = self.feature_generation_user()
      content_engagement_features = self.feature_generation_content_engagement_value()
      generated_content_features = self.feature_generation_content_one_hot_encoding()
      self.results = pd.merge(
          generated_content_features,
          content_engagement_features,
          on='content_id',
          how='left'
      ).fillna(0)
      self.results['user_id'] = user_attr['user_id'].iloc[0]
      self.results = pd.merge(
          self.results,
          user_attr,
          on='user_id'
      )

    def threshold(self):
        raise NotImplementedError("you need to implement")

    def coefficients(self):
        return {
            'content_likes': -0.00023,
            'content_dislikes': 0.001026,
            'content_engagement_time_avg': 3.68948e-7,
            'user_likes': 4.275072e-6,
            'user_dislikes': -1.988219e-6,
            'user_engagement_time_avg': 3.185241e-7,
        }

    def get_columns(self):
      cols = list(self.coefficients().keys())
      for (categories, _coefficients), col_name in self.one_hot_encoding_functions():
          for category, coefficient in zip(categories + ['other'], _coefficients):
            cols.append(col_name + "_" + str(category))
      return cols

    def run_linear_model(self):
        coeffs = self.coefficients()
        for (categories, _coefficients), col_name in self.one_hot_encoding_functions():
          for category, coefficient in zip(categories + ['other'], _coefficients):
            coeffs[col_name + "_" + str(category)] = coefficient

        self.results['linear_output'] = -0.026356
        for col_name, _coefficient in coeffs.items():
            self.results['linear_output'] += self.results[col_name] * _coefficient
        return self.results[self.results['linear_output'] >= self.threshold()]['content_id'].values

    def filter_content_ids(self, user_id, content_ids):
      self.gather_data(user_id, content_ids)
      self.feature_eng()
      return self.run_linear_model()

"""# Your Implementation - Example Here, Must Modify"""

class DataCollectorCharlie(DataCollector):
  def artist_styles_one_hot(self):
    return [
        "medieval", "oil_on_canvas", "scifi", "leonardo_da_vinci", "movie: Batman", "movie: Gold"
    ], [
        -0.002887, -0.001683, -0.002155, 0.000314, 0.002131, 0.004903, -0.000623
    ]

  def sources_one_hot(self):
    return [
        "human_prompts", "r/EarthPorn", "r/SimplePrompts", "r/Cyberpunk", "r/whoahdude", "William Shakespeare"
    ], [
        0.000101, 0.00494, -0.001134, -0.000446, -0.001773, -0.000422, -0.001264
    ]

  def num_inference_steps_one_hot(self):
    return [
        100, 50
    ], [
        -0.00031, -0.000965, -0.000655
    ]

  def one_hot_encoding_functions(self):
    return zip(
        [self.artist_styles_one_hot(), self.sources_one_hot(), self.num_inference_steps_one_hot()],
        ['artist_style', 'source', 'num_inference_steps']
    )

  def threshold(self):
    return 1.19833

"""# Example For Use In Production"""

data_collector = DataCollectorCharlie()
random_content_ids = pd.read_csv('sample_data/generated_content_metadata.csv', sep="\t")['content_id'].values
data_collector.filter_content_ids(1, random_content_ids)

"""# Training"""

data_collector = DataCollectorCharlie()
data_collector.gather_training_data()
training_data = data_collector.feature_eng_training()

def get_Y(engagement_data: pd.DataFrame) -> pd.DataFrame:
    """Engineers target variable.
    Args
      data (pd.DataFrame): Engagement data.
    Returns
      pd.DataFrame: Dataframe of 3 columns; 'user_id', 'content_id', 'score',
        where 'score' being the target variable that you want to predict.
    """
    # Dummy target dataframe. Your output dataframe should have 3 columns; 'user_id', 'content_id', 'score'
    # Where 'score' being the target variable that you want to predict.
    engagement_data_copy = engagement_data.copy()
    weights = {"Like": 5, "MsecEngagedWith": 0.0005}
    engagement_data_copy["score"] = np.where(engagement_data_copy["engagement_type"] == "Like",
                                             engagement_data_copy["engagement_value"] * weights["Like"],
                                             engagement_data_copy["engagement_value"] * weights["MsecEngagedWith"])
    target_df = engagement_data_copy.groupby(["user_id", "content_id"])["score"].mean().to_frame().reset_index()
    s_scaler = StandardScaler()
    target_df["score"] = s_scaler.fit_transform(target_df[["score"]])

    # DO NOT CHANGE THIS. This step ensures that each row of the target variable (X)
    # corresponds to the correct row of features (y).
    target_df = pd.merge(
        training_data[['user_id', 'content_id']],
        target_df,
        on=['user_id', 'content_id'],
        how='left'
    )
    return target_df["score"]

engagement_data = pd.read_csv('sample_data/engagement.csv', sep="\t")
X = training_data[data_collector.get_columns()]
y = get_Y(engagement_data)

# training
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score

# Split data into train and test: Add/change  other parametersas you wish
# Also, feel free to use cross validation
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Depending on what your target variable y looks like, you have to choose a suitable model.
model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(f"MSE: {np.mean((y_pred - y_test)**2)}")

"""# What You Need"""

print("{")
for x, y in zip(model.feature_names_in_, model.coef_):
  print(f"\t{x}: {y},")
print("}")

"""# Policy Filtering 1"""

def policy_filter_one(training_data, content_id):
    desired_styles = ['human_prompts', 'r/EarthPorn', 'r/Showerthoughts']
    artist_style = training_data[training_data['content_id'] == content_id]['artist_style'].values[0]
    if artist_style in desired_styles:
      return True
    else:
      return False


policy_filter_one(
    training_data[training_data['content_id'] == random_content_ids[0]],
    random_content_ids[0]
)

"""# Policy Filtering 2"""

def policy_filter_two(training_data, content_id):
  filtered_content_ids = []
  net_likes_threshold = 4
  training_data = training_data.merge(engagement_data[["content_id", "engagement_type", "engagement_value", ]], on="content_id", how="left")
  net_likes = training_data[(training_data['content_id'] == content_id) & (training_data["engagement_type"] == "Like")]['engagement_value'].sum()
  if net_likes >= net_likes_threshold:
    return True
  else:
    return False


policy_filter_two(
    training_data[training_data['content_id'] == random_content_ids[0]],
    random_content_ids[0]
)

