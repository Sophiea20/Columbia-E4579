{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBiX1FEG1nXj"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1572,
     "status": "ok",
     "timestamp": 1698859317795,
     "user": {
      "displayName": "Poch Laohrenu",
      "userId": "13757591209205873974"
     },
     "user_tz": 240
    },
    "id": "NkuqgSAjwlnY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "#from google.colab import drive\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSXqZ70o1omv"
   },
   "source": [
    "# DataCollector - Do Not Modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1698873712416,
     "user": {
      "displayName": "Poch Laohrenu",
      "userId": "13757591209205873974"
     },
     "user_tz": 240
    },
    "id": "4CMmBKnre2Hv"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.sql.schema import ScalarElementColumnDefault\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "class DataCollector:\n",
    "\n",
    "    def artist_styles_one_hot(self):\n",
    "        raise NotImplementedError(\n",
    "            \"you need to implement this, needs to be two lists, one for string one for coefficient, coefficient list is one larger to account for 'other'\"\n",
    "            \"Coefficient is from the model after training, so to prepare training data, you can put dummy number first, then replace it later after model has been trained\"\n",
    "        )\n",
    "\n",
    "    def sources_one_hot(self):\n",
    "        raise NotImplementedError(\n",
    "            \"you need to implement this, needs to be two lists, one for string one for coefficient, coefficient list is one larger to account for 'other'\"\n",
    "            \"Coefficient is from the model after training, so to prepare training data, you can put dummy number first, then replace it later after model has been trained\"\n",
    "        )\n",
    "\n",
    "    def num_inference_steps_one_hot(self):\n",
    "        raise NotImplementedError(\n",
    "            \"you need to implement this, needs to be two lists, one for string one for coefficient, coefficient list is one larger to account for 'other'\"\n",
    "            \"Coefficient is from the model after training, so to prepare training data, you can put dummy number first, then replace it later after model has been trained\"\n",
    "        )\n",
    "\n",
    "    def one_hot_encoding_functions(self):\n",
    "        return zip(\n",
    "            [self.artist_styles_one_hot(), self.sources_one_hot(), self.num_inference_steps_one_hot()],\n",
    "            ['artist_style', 'source', 'num_inference_steps']\n",
    "        )\n",
    "\n",
    "    def custom_aggregation(self, prefix, data):\n",
    "        result = {\n",
    "            f'{prefix}_likes': np.sum((data['engagement_type'] == 'Like') & (data['engagement_value'] == 1)),\n",
    "            f'{prefix}_dislikes': np.sum((data['engagement_type'] == 'Like') & (data['engagement_value'] == -1)),\n",
    "            f'{prefix}_engagement_time_avg': data[data['engagement_type'] == 'MillisecondsEngagedWith']['engagement_value'].mean(),\n",
    "        }\n",
    "        return pd.Series(result)\n",
    "\n",
    "    def feature_generation_user(self):\n",
    "        return self.user_data.groupby('user_id').apply(lambda data: self.custom_aggregation('user', data)).reset_index()\n",
    "\n",
    "    def feature_generation_content_one_hot_encoding(self):\n",
    "        for (categories, _coefficient), col_name in self.one_hot_encoding_functions():\n",
    "            transformed_col = self.generated_content_metadata_data[col_name].apply(lambda x: x if x in categories else 'other').to_frame()\n",
    "            encoder = OneHotEncoder(categories=[categories + ['other']], sparse=False)\n",
    "            encoded_data = encoder.fit_transform(transformed_col)\n",
    "            encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out([col_name]))\n",
    "            for col in encoded_df.columns:\n",
    "              self.generated_content_metadata_data[col] = encoded_df[col]\n",
    "        return self.generated_content_metadata_data\n",
    "\n",
    "    def feature_generation_content_engagement_value(self):\n",
    "        return self.engagement_data.groupby('content_id').apply(\n",
    "            lambda data: self.custom_aggregation('content', data)\n",
    "        ).reset_index()\n",
    "\n",
    "    def feature_generation(self):\n",
    "      self.feature_generation_user()\n",
    "      self.feature_generation_content_one_hot_encoding()\n",
    "      self.feature_generation_content_engagement_value()\n",
    "\n",
    "    def get_engagement_data(self, content_ids):\n",
    "      df = pd.read_csv('sample_data/engagement.csv', sep=\"\\t\")\n",
    "      return df[df['content_id'].isin(content_ids)]\n",
    "\n",
    "    def get_generated_content_metadata_data(self, content_ids):\n",
    "      df = pd.read_csv('sample_data/generated_content_metadata.csv', sep=\"\\t\")\n",
    "      return df[df['content_id'].isin(content_ids)]\n",
    "\n",
    "    def get_user_data(self, user_id):\n",
    "      df = pd.read_csv('sample_data/engagement.csv', sep=\"\\t\")\n",
    "      return df[df['user_id'] == user_id]\n",
    "\n",
    "    def gather_data(self, user_id, content_ids):\n",
    "      self.engagement_data = self.get_engagement_data(content_ids)\n",
    "      self.generated_content_metadata_data = self.get_generated_content_metadata_data(content_ids)\n",
    "      self.user_data = self.get_user_data(user_id)\n",
    "\n",
    "    def gather_training_data(self):\n",
    "      self.engagement_data = pd.read_csv('sample_data/engagement.csv', sep=\"\\t\")\n",
    "      self.generated_content_metadata_data = pd.read_csv('sample_data/generated_content_metadata.csv', sep=\"\\t\")\n",
    "      self.user_data = pd.read_csv('sample_data/engagement.csv', sep=\"\\t\")\n",
    "\n",
    "    def feature_eng_training(self):\n",
    "      user_attr = self.feature_generation_user()\n",
    "      content_engagement_features = self.feature_generation_content_engagement_value()\n",
    "      generated_content_features = self.feature_generation_content_one_hot_encoding()\n",
    "\n",
    "      interaction_pairs = self.engagement_data[\n",
    "          ['user_id', 'content_id']].drop_duplicates()\n",
    "\n",
    "      self.training_results = pd.merge(\n",
    "          interaction_pairs,\n",
    "          user_attr,\n",
    "          on='user_id',\n",
    "          how='left'\n",
    "      ).fillna(0)\n",
    "\n",
    "      content_results = pd.merge(\n",
    "          generated_content_features,\n",
    "          content_engagement_features,\n",
    "          on='content_id',\n",
    "          how='left'\n",
    "      ).fillna(0)\n",
    "\n",
    "      self.training_results = pd.merge(\n",
    "          self.training_results,\n",
    "          content_results,\n",
    "          on='content_id',\n",
    "          how='left'\n",
    "      ).fillna(0)\n",
    "\n",
    "      return self.training_results\n",
    "\n",
    "    def feature_eng(self):\n",
    "      user_attr = self.feature_generation_user()\n",
    "      content_engagement_features = self.feature_generation_content_engagement_value()\n",
    "      generated_content_features = self.feature_generation_content_one_hot_encoding()\n",
    "      self.results = pd.merge(\n",
    "          generated_content_features,\n",
    "          content_engagement_features,\n",
    "          on='content_id',\n",
    "          how='left'\n",
    "      ).fillna(0)\n",
    "      self.results['user_id'] = user_attr['user_id'].iloc[0]\n",
    "      self.results = pd.merge(\n",
    "          self.results,\n",
    "          user_attr,\n",
    "          on='user_id'\n",
    "      )\n",
    "\n",
    "    def threshold(self):\n",
    "        raise NotImplementedError(\"you need to implement\")\n",
    "\n",
    "    def coefficients(self):\n",
    "        return {\n",
    "            'content_likes': 0.0,\n",
    "            'content_dislikes': 0.0,\n",
    "            'content_engagement_time_avg': 0.0,\n",
    "\n",
    "            'user_likes': 0.0,\n",
    "            'user_dislikes': 0.0,\n",
    "            'user_engagement_time_avg': 0.0,\n",
    "        }\n",
    "\n",
    "    def get_columns(self):\n",
    "      cols = list(self.coefficients().keys())\n",
    "      for (categories, _coefficients), col_name in self.one_hot_encoding_functions():\n",
    "          for category, coefficient in zip(categories + ['other'], _coefficients):\n",
    "            cols.append(col_name + \"_\" + str(category))\n",
    "      return cols\n",
    "\n",
    "    def run_linear_model(self):\n",
    "        coeffs = self.coefficients()\n",
    "        for (categories, _coefficients), col_name in self.one_hot_encoding_functions():\n",
    "          for category, coefficient in zip(categories + ['other'], _coefficients):\n",
    "            coeffs[col_name + \"_\" + str(category)] = coefficient\n",
    "\n",
    "        self.results['linear_output'] = 0.0\n",
    "        for col_name, _coefficient in coeffs.items():\n",
    "            self.results['linear_output'] += self.results[col_name] * _coefficient\n",
    "        return self.results[self.results['linear_output'] >= self.threshold()]['content_id'].values\n",
    "\n",
    "    def filter_content_ids(self, user_id, content_ids):\n",
    "      self.gather_data(user_id, content_ids)\n",
    "      self.feature_eng()\n",
    "      return self.run_linear_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA9SBLao1s4f"
   },
   "source": [
    "# Your Implementation - Example Here, Must Modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollectorFoxtrot(DataCollector):\n",
    "    def coefficients(self):\n",
    "        return {\n",
    "            'content_likes': 0.001327374532861648,\n",
    "            'content_dislikes': -0.0025596352777526488,\n",
    "            'content_engagement_time_avg': -1.1359359152350807e-10,\n",
    "\n",
    "            'user_likes': 6.150439880704869e-06,\n",
    "            'user_dislikes': -7.111467664335854e-06,\n",
    "            'user_engagement_time_avg':  -4.198888030205803e-09,\n",
    "        }\n",
    "    \n",
    "    def artist_styles_one_hot(self):\n",
    "        return ['medieval','anime','studio','oil_on_canvas','unreal_engine', 'edward_hopper','shepard_fairey',], [\n",
    "                0.00297867123567935, -0.00013583832416803058, 0.0011875140593540518, -0.0008508512202171477, \n",
    "                -0.0011910319715602647, -0.0009473978012908358, 0.0015827575817499537, -0.002623823559547084]\n",
    "\n",
    "    def sources_one_hot(self):\n",
    "        return ['human_prompts','r/EarthPorn','r/Showerthoughts','r/scifi','r/pics','r/educationalgifs','r/Damnthatsinteresting',], [\n",
    "                0.003435938296346102, 0.002055277050584419, -0.0016261067852378207, -1.3188188316680503e-05, -0.0014536933603578126,\n",
    "                -0.000850669870952967, -0.0013183930372200371, -0.00022916410484521567]\n",
    "\n",
    "    def num_inference_steps_one_hot(self):\n",
    "        return [20, 50, 100], [0.0003241249776025939, 0.00013478447734715756, 0.0015154732174153412, -0.0019743826723650913]\n",
    "\n",
    "    \n",
    "    def threshold(self):\n",
    "        return 0.005\n",
    "\n",
    "\n",
    "    def policy_filter_one(self):\n",
    "        temp = self.engagement_data.loc[self.engagement_data[\"engagement_type\"] == \"Like\"].groupby(\"content_id\")['engagement_value'].sum().rename('score', inplace=True).to_frame()\n",
    "        temp = temp.loc[temp['score'] > 0]\n",
    "        self.results = self.results[self.results['content_id'].isin(temp.index)]\n",
    "        self.results = self.results.reset_index(drop=True)\n",
    "        \n",
    "    def policy_filter_two(self):\n",
    "        temp2 = self.engagement_data.loc[self.engagement_data[\n",
    "            'engagement_type']=='MillisecondsEngagedWith'].groupby(\n",
    "            'content_id')['engagement_value'].count().rename(\n",
    "            'count',inplace=True).to_frame().sort_values(by='count').iloc[50:]\n",
    "        self.results = self.results[self.results['content_id'].isin(temp2.index)].reset_index(drop=True)\n",
    "\n",
    "    def filter_content_ids(self, user_id, content_ids):\n",
    "        self.gather_data(user_id, content_ids)\n",
    "        self.feature_eng()\n",
    "        self.policy_filter_one() # policy one used here\n",
    "        self.policy_filter_two() # policy two used here\n",
    "        return self.run_linear_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjbyuksB1zBB"
   },
   "source": [
    "# Example For Use In Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 649,
     "status": "ok",
     "timestamp": 1698526162116,
     "user": {
      "displayName": "Poch Laohrenu",
      "userId": "13757591209205873974"
     },
     "user_tz": 240
    },
    "id": "mye5Pgzee6Bw",
    "outputId": "28ec39c1-8aaf-4f8d-a1a4-991701f3e356",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taru/.virtualenvs/4579/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/home/taru/.virtualenvs/4579/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/home/taru/.virtualenvs/4579/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collector = DataCollectorFoxtrot()\n",
    "random_content_ids = pd.read_csv('sample_data/generated_content_metadata.csv', sep=\"\\t\")['content_id'].iloc[:100].values\n",
    "data_collector.filter_content_ids(1, random_content_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMev-xwJ11d5"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120052,
     "status": "ok",
     "timestamp": 1698873838932,
     "user": {
      "displayName": "Poch Laohrenu",
      "userId": "13757591209205873974"
     },
     "user_tz": 240
    },
    "id": "yvMO4tMXe9io",
    "outputId": "36cdcee1-6acc-4611-a008-a95aca4b42ee"
   },
   "outputs": [],
   "source": [
    "#@title get training data\n",
    "data_collector = DataCollectorFoxtrot()\n",
    "data_collector.gather_training_data()\n",
    "training_data = data_collector.feature_eng_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "923Jz78HK1Yx"
   },
   "outputs": [],
   "source": [
    "#@title code to implement\n",
    "def get_Y(engagement_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Engineers taget variable.\n",
    "    Args\n",
    "      data (pd.DataFrame): Engagement data.\n",
    "    Returns\n",
    "      pd.DataFrame: Dataframe of 3 columns; 'user_id', 'content_id', 'score',\n",
    "        where 'score' being the target variable that you want to predict.\n",
    "    \"\"\"\n",
    "    # Dummy target dataframe. Your output dataframe should have 3 columns; 'user_id', 'content_id', 'score'\n",
    "    # Where 'score' being the target variable that you want to predict.\n",
    "\n",
    "    # Apply the rules to calculate a new engagement score\n",
    "    engagement_data['new_engagement_value'] = engagement_data.apply(\n",
    "        lambda row: row['engagement_value'] if row['engagement_type'] == 'Like' else \n",
    "        (1 if 717 <= row['engagement_value'] <= 2483 else 0), axis=1\n",
    "    )\n",
    "    \n",
    "    # Group by user_id and content_id, then sum the new engagement values\n",
    "    target_df = engagement_data.groupby(\n",
    "        ['user_id', 'content_id']\n",
    "    )['new_engagement_value'].sum().rename('score', inplace=True).to_frame().reset_index()\n",
    "    \n",
    "    # Min-max normalization of the score\n",
    "    min_score = target_df['score'].min()\n",
    "    max_score = target_df['score'].max()\n",
    "    target_df['score'] = (target_df['score'] - min_score) / (max_score - min_score)\n",
    "    \n",
    "\n",
    "    # DO NOT CHANGE THIS. This step ensures that each row of the target variable (X)\n",
    "    # corresponds to the correct row of features (y).\n",
    "    target_df = pd.merge(\n",
    "          training_data[['user_id', 'content_id']],\n",
    "          target_df,\n",
    "          on=['user_id', 'content_id'],\n",
    "          how='left'\n",
    "      )\n",
    "\n",
    "    return target_df['score']\n",
    "\n",
    "engagement_data = pd.read_csv('sample_data/engagement.csv', sep=\"\\t\")\n",
    "X = training_data[data_collector.get_columns()]\n",
    "y = get_Y(engagement_data)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1698526444915,
     "user": {
      "displayName": "Poch Laohrenu",
      "userId": "13757591209205873974"
     },
     "user_tz": 240
    },
    "id": "foPA5J9-yiOO",
    "outputId": "d2da2c29-4414-4ba1-8814-18507d755fbc"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data into train and test: Add/change  other parametersas you wish\n",
    "# Also, feel free to use cross validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Depending on what your target variable y looks like, you have to choose a suitable model.\n",
    "# Here, I assume y is binary, and so I use Logistic Regression.\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1698526451158,
     "user": {
      "displayName": "Poch Laohrenu",
      "userId": "13757591209205873974"
     },
     "user_tz": 240
    },
    "id": "6fTHbJmI1zPK",
    "outputId": "e0beda77-0a94-411f-82ce-dca9cb8d5ff7"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"MSE: {np.mean((y_pred - y_test)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEuIWDy39ZQP"
   },
   "source": [
    "# What You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1698526488012,
     "user": {
      "displayName": "Poch Laohrenu",
      "userId": "13757591209205873974"
     },
     "user_tz": 240
    },
    "id": "rrNKB1u49YGr",
    "outputId": "ae7125f7-5539-4aff-daff-6d7556f6f4b1"
   },
   "outputs": [],
   "source": [
    "print(\"{\")\n",
    "for x, y in zip(model.feature_names_in_, model.coef_):\n",
    "  print(f\"\\t{x}: {y},\")\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3LGCsWl1j7_"
   },
   "source": [
    "# Policy Filtering 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 183,
     "status": "ok",
     "timestamp": 1697937790179,
     "user": {
      "displayName": "Kenneth Goodman",
      "userId": "12891531330941698958"
     },
     "user_tz": 240
    },
    "id": "eyEArF5W1jhP",
    "outputId": "6567328d-d8e4-44d5-ad56-976efe21daad"
   },
   "outputs": [],
   "source": [
    "def policy_filter_one(self):\n",
    "    temp = self.engagement_data.loc[self.engagement_data[\"engagement_type\"] == \"Like\"].groupby(\"content_id\")['engagement_value'].sum().rename('score', inplace=True).to_frame()\n",
    "    temp = temp.loc[temp['score'] > 0]\n",
    "    self.results = self.results[self.results['content_id'].isin(temp.index)]\n",
    "    self.results = self.results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhh1vy1c1lbs"
   },
   "source": [
    "# Policy Filtering 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1697937797242,
     "user": {
      "displayName": "Kenneth Goodman",
      "userId": "12891531330941698958"
     },
     "user_tz": 240
    },
    "id": "PaFGFkfd2ePw",
    "outputId": "235ddd84-a04f-4638-e1db-64048b32b9b1"
   },
   "outputs": [],
   "source": [
    "def policy_filter_two(self):\n",
    "    temp2 = self.engagement_data.loc[self.engagement_data[\n",
    "        'engagement_type']=='MillisecondsEngagedWith'].groupby(\n",
    "        'content_id')['engagement_value'].count().rename(\n",
    "        'count',inplace=True).to_frame().sort_values(by='count').iloc[50:]\n",
    "    self.results = self.results[self.results['content_id'].isin(temp2.index)].reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
