# -*- coding: utf-8 -*-
"""MyTwoTower.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fuIc-k-fSn46NLSP2PWgaTpEjoAaix-b
"""

from google.colab import drive
import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler
import torch
import pickle
import traceback
import random

drive.mount('/content/drive')

df = pd.read_csv('drive/MyDrive/E4579 Fall 2023/columbia_data_grouping.csv')
#with open("drive/MyDrive/E4579 Fall 2023/id_to_pickle_dict.pkl", "rb") as f:
#    prompt_to_embedding = pickle.load(f)
#prompt_embedding = pd.DataFrame(prompt_to_embedding.items(), columns=["content_id", "prompt_embedding"])
#df = df.merge(prompt_embedding, on="content_id")
#del prompt_embedding
index_to_content_id = df['content_id'].to_dict()
df

df = pd.read_csv('drive/MyDrive/E4579 Fall 2023/columbia_data_grouping.csv')

mostfreq_10 = df.assign(counter=df.index).groupby('content_id').agg({'group':'last',
                              'counter':'size'}).sort_values('counter',ascending=False)
mostfreq_10 = (lambda x:x[x.counter>10])(mostfreq_10)
mostfreq_10_eff = mostfreq_10.iloc[:3378]

s3url = pd.read_csv('drive/MyDrive/E4579 Fall 2023/s3url.csv')
s3url_core = s3url[s3url.id.isin(mostfreq_10_eff.index)]


with open("drive/MyDrive/E4579 Fall 2023/clip_embed.pkl", "rb") as f:
    clip_embed = pickle.load(f)

mostfreq_10_eff

clip_embedding = mostfreq_10_eff
clip_embedding['vecs'] = list(clip_embed)

clip_embedding

clip_embedding.to_csv('drive/MyDrive/E4579 Fall 2023/clip_embeding.csv')

print(f'Pre-merge len of df {len(df)}')
df = df.merge(clip_embedding, on="content_id")
print(f'Post-merge len of df {len(df)}')
#del clip_embedding

#!pip install git+https://github.com/openai/CLIP.git



import os
#import clip
#import torch

# Load the model
#device = "cuda" if torch.cuda.is_available() else "cpu"
#clip, preprocess = clip.load('ViT-B/32', device)



#s3url = pd.read_csv('drive/MyDrive/E4579 Fall 2023/s3url.csv')
#s3url_core = s3url[s3url.id.isin(mostfreq_10.index)]

# preprocess df

TOP_ARTIST_STYLES = 30
TOP_SOURCES = 30
TOP_SEEDS = 14
TOP_CONTENT = 251
PROMPT_EMBEDDING_LENGTH = 512

# Get the top artist styles, sources, and seeds
top_artist_styles = df['artist_style'].value_counts().nlargest(TOP_ARTIST_STYLES).index.tolist()
top_sources = df['source'].value_counts().nlargest(TOP_SOURCES).index.tolist()
top_seeds = df['seed'].value_counts().nlargest(TOP_SEEDS).index.tolist()

import json

# Create dictionaries for each list
top_data = {
    'top_artist_styles': top_artist_styles,
    'top_sources': top_sources,
    'top_seeds': top_seeds
}

# Save the data to a JSON file
with open('drive/MyDrive/E4579 Fall 2023/top_data.json', 'w') as file:
    json.dump(top_data, file)

# Replace less frequent artist styles, sources, and seeds with 'other'
df['artist_style'] = df['artist_style'].apply(lambda x: x if x in top_artist_styles else 'other')
df['source'] = df['source'].apply(lambda x: x if x in top_sources else 'other')
df['seed'] = df['seed'].apply(lambda x: str(x) if x in top_seeds else 'other')

# One-hot encode categorical features
encoder = OneHotEncoder()
content_onehot = encoder.fit_transform(df[['artist_style', 'model_version', 'seed', 'source']])
content_onehot_df = pd.DataFrame(content_onehot.toarray(), columns=encoder.get_feature_names_out(['artist_style', 'model_version', 'seed', 'source']))
df = pd.concat([df, content_onehot_df], axis=1)

# unpack clip columns
clip_columns = [f"clip_embedding_{i}" for i in range(PROMPT_EMBEDDING_LENGTH)]
df[clip_columns] = pd.DataFrame(df['vecs'].tolist(), index=df.index)
df = df.drop('vecs', axis=1)


# Normalizing linear features
scaler = StandardScaler()
df[['guidance_scale', 'num_inference_steps']] = scaler.fit_transform(df[['guidance_scale', 'num_inference_steps']])

# Compute top N content pieces based on engagement_value
from collections import defaultdict
top_n_content = df.groupby('content_id')['engagement_value'].count().nlargest(TOP_CONTENT).index.tolist()
with open('drive/MyDrive/E4579 Fall 2023/top_n_content.json', 'w') as file:
    json.dump(top_n_content, file)

user_columns = (
        [f'ms_engaged_{i}' for i in range(TOP_CONTENT)] +
        [f'like_vector_{i}' for i in range(TOP_CONTENT)] +
        [f'dislike_vector_{i}' for i in range(TOP_CONTENT)]
    )

# content_columns depends on df's column names!
content_columns = (
        list(filter(lambda x: 'artist_style_' in x, df.columns)) +
        #list(filter(lambda x: 'model_version_' in x, df.columns)) +
        list(filter(lambda x: 'source_' in x, df.columns)) +
        list(filter(lambda x: 'seed_' in x, df.columns)) +
        list(filter(lambda x: 'clip_embedding_' in x, df.columns)) +
        ['content_id','guidance_scale', 'num_inference_steps']
    )

print(f'User features num = {len(user_columns)},Content features num = {len(content_columns)}')

print(len(list(filter(lambda x: 'artist_style_' in x, df.columns))))
print(len(list(filter(lambda x: 'source_' in x, df.columns))))
print(len(list(filter(lambda x: 'seed_' in x, df.columns))))

list(filter(lambda x: 'artist_style_' in x, df.columns))

def df_to_user_tensor_default(df):
    # User: Construct groupby-like columns for each user using
    def aggregate_engagement(group):
        # Summing millisecond engagement values
        millisecond_engagement_sum = group.loc[group['engagement_type'] != 'Like', 'engagement_value'].sum()

        # Counting likes and dislikes
        likes_count = group.loc[(group['engagement_type'] == 'Like') & (group['engagement_value'] == 1)].shape[0]
        dislikes_count = group.loc[(group['engagement_type'] == 'Like') & (group['engagement_value'] == -1)].shape[0]

        return pd.Series({
            'millisecond_engagement_sum': millisecond_engagement_sum,
            'likes_count': likes_count,
            'dislikes_count': dislikes_count
        })

    # Group by user_id and content_id, then apply the function
    engagement_aggregate = df[df['content_id'].isin(top_n_content)].groupby(['user_id', 'content_id']).apply(aggregate_engagement).reset_index()


    user_vector_dict = defaultdict(lambda: {
        'millisecond_engaged_vector': np.zeros(len(top_n_content)),
        'like_vector': np.zeros(len(top_n_content)),
        'dislike_vector': np.zeros(len(top_n_content))
    })
    # Now, populate your user_vector_dict
    for _, row in engagement_aggregate.iterrows():
        user_id = row['user_id']
        content_id = row['content_id']
        idx = top_n_content.index(content_id)

        user_vector_dict[user_id]['millisecond_engaged_vector'][idx] = row['millisecond_engagement_sum']
        user_vector_dict[user_id]['like_vector'][idx] = row['likes_count']
        user_vector_dict[user_id]['dislike_vector'][idx] = row['dislikes_count']

    # Convert to DataFrame
    user_vector_df = pd.DataFrame.from_dict(user_vector_dict, orient='index')
    del user_vector_dict

    # User - UNPACK vector columns into individual columns
    millisecond_columns = [f"ms_engaged_{i}" for i in range(TOP_CONTENT)]
    like_columns = [f"like_vector_{i}" for i in range(TOP_CONTENT)]
    dislike_columns = [f"dislike_vector_{i}" for i in range(TOP_CONTENT)]

    user_vector_df[millisecond_columns] = pd.DataFrame(user_vector_df['millisecond_engaged_vector'].tolist(), index=user_vector_df.index)
    user_vector_df[like_columns] = pd.DataFrame(user_vector_df['like_vector'].tolist(), index=user_vector_df.index)
    user_vector_df[dislike_columns] = pd.DataFrame(user_vector_df['dislike_vector'].tolist(), index=user_vector_df.index)

    # Drop the original vector columns
    user_vector_df.drop(['millisecond_engaged_vector', 'like_vector', 'dislike_vector'], axis=1, inplace=True)

    # Join User Vector To Df
    print(f'Before merge, len of df is {len(df)}')
    df = df.merge(
        user_vector_df.reset_index().rename(columns={'index': 'user_id'}),
        on='user_id'
    )
    print(f'After merge, len of df is {len(df)}')

    # Extract user and Extract content, turn into tensor
    user_features = df[user_columns]
    print(f'len of df is {len(df)}, len of user_features is {len(user_features)}')
    user_features.to_csv('drive/MyDrive/E4579 Fall 2023/user_features.csv')
    user_features_tensor = torch.FloatTensor(user_features.values)
    del user_features

    return df, user_features_tensor

def df_to_user_tensor(df):
    from collections import defaultdict
    import json

    PROMPT_EMBEDDING_LENGTH = 512
    TOP_CONTENT = 251
    with open('top_n_content.json', 'r') as file:
        top_n_content = json.load(file)

    df.created_date = pd.to_datetime(df.created_date)
    df['contentCmltLike'] = ((df.engagement_type == 'Like') & (df.engagement_value == 1)).groupby(df.content_id).cumsum()
    df['contentCmltDislike'] = ((df.engagement_type == 'Like') & (df.engagement_value == -1)).groupby(df.content_id).cumsum()
    df['contentCmltEngagement'] = ((df.engagement_type == 'MillisecondsEngagedWith')).groupby(df.content_id).cumsum()
    df['userCmltLike'] = ((df.engagement_type == 'Like') & (df.engagement_value == 1)).groupby(df.user_id).cumsum()
    df['userCmltLike'] = ((df.engagement_type == 'Like') & (df.engagement_value == 1)).groupby(df.user_id).cumsum()
    df['userCmltDislike'] = ((df.engagement_type == 'Like') & (df.engagement_value == -1)).groupby(df.user_id).cumsum()
    df['userCmltEngagement'] = ((df.engagement_type == 'MillisecondsEngagedWith')).groupby(df.user_id).cumsum()
    df['userStartUsing'] = df.groupby('user_id')['created_date'].transform('min')
    df['timeSinceStart'] = (df.created_date - df.userStartUsing).dt.total_seconds()

    merger = lambda df:pd.merge(
        df[df.engagement_type=='MillisecondsEngagedWith'][['user_id','content_id','engagement_value',
                                                      'contentCmltLike','contentCmltDislike','contentCmltEngagement',
                                                      'userCmltLike','userCmltDislike','userCmltEngagement',
                                                      'timeSinceStart']],
        df[df.engagement_type=='Like'][['content_id','engagement_value']],
        on='content_id',
        how='left').fillna(0)

    All_Liked_content = df[(df.engagement_type=='Like')&(df.engagement_value==1)].groupby('content_id', as_index=False).agg({"engagement_value":"sum"})
    # Liked_content.plot()
    # popular content threshole is 10
    Popular_content = All_Liked_content[All_Liked_content.engagement_value > 10]
    Popular_content

    All_Disliked_content = df[(df.engagement_type=='Like')&(df.engagement_value==-1)].groupby('content_id', as_index=False).agg({"engagement_value":"sum"})
        # Disliked_content.plot()
    # unfavorable content threshold is -6
    Unfavorable_content = All_Disliked_content[All_Disliked_content.engagement_value < -6]
    Unfavorable_content

    df['popular_content'] = np.where(df['content_id'].isin(Popular_content['content_id']),1,0)
    df['unfavorable_content'] = np.where(df['content_id'].isin(Unfavorable_content['content_id']),1,0)

    unfavorable_saw = df[(df.unfavorable_content==1)].groupby('user_id', as_index=False).agg({"engagement_value":"count"})
    unfavorable_saw

    popular_saw = df[(df.popular_content==1)].groupby('user_id', as_index=False).agg({"engagement_value":"count"})
    popular_saw

    #define indie scores for users
    indie = df[(df.engagement_type=='Like')&(df.engagement_value==1)&(df.unfavorable_content==1)].groupby('user_id', as_index=False).agg({"engagement_value":"sum"})
    #indie['user_id'].isin(unfavorable_saw['user_id'])
    indie = pd.merge(indie,unfavorable_saw,on='user_id',how='left')
    indie['score'] = indie['engagement_value_x']/indie['engagement_value_y']
    #indie

    #define vigilante scores for users
    vigilante = df[(df.engagement_type=='Like')&(df.engagement_value==-1)&(df.popular_content==1)].groupby('user_id', as_index=False).agg({"engagement_value":"sum"})
    #vigilante['user_id'].isin(popular_saw['user_id'])
    vigilante = pd.merge(vigilante,popular_saw,on='user_id',how='left')
    vigilante['score'] = -vigilante['engagement_value_x']/vigilante['engagement_value_y']
    #vigilante

    #define basic scores for users
    basic_like = df[(df.engagement_type=='Like')&(df.engagement_value==1)&(df.popular_content==1)].groupby('user_id', as_index=False).agg({"engagement_value":"sum"})
    #basic_like['user_id'].isin(popular_saw['user_id'])
    basic_like = pd.merge(basic_like,popular_saw,on='user_id',how='left')
    basic_like['score'] = basic_like['engagement_value_x']/basic_like['engagement_value_y']
    #basic_like

    basic_dislike = df[(df.engagement_type=='Like')&(df.engagement_value==-1)&(df.unfavorable_content==1)].groupby('user_id', as_index=False).agg({"engagement_value":"sum"})
    #basic_dislike['user_id'].isin(unfavorable_saw['user_id'])
    basic_dislike = pd.merge(basic_dislike,unfavorable_saw,on='user_id',how='left')
    basic_dislike['score'] = -basic_dislike['engagement_value_x']/basic_dislike['engagement_value_y']
    #basic_dislike


    user_vector_dict = defaultdict(lambda: {
        'millisecond_engaged_vector': np.zeros(len(top_n_content)),
        'like_vector': np.zeros(len(top_n_content)),
        'dislike_vector': np.zeros(len(top_n_content))
    })

    # Initialize vectors for each user
    def aggregate_engagement(group):
        # Summing millisecond engagement values
        millisecond_engagement_sum = group.loc[group['engagement_type'] != 'Like', 'engagement_value'].sum()

        # Counting likes and dislikes
        likes_count = group.loc[(group['engagement_type'] == 'Like') & (group['engagement_value'] == 1)].shape[0]
        dislikes_count = group.loc[(group['engagement_type'] == 'Like') & (group['engagement_value'] == -1)].shape[0]

        return pd.Series({
            'millisecond_engagement_sum': millisecond_engagement_sum,
            'likes_count': likes_count,
            'dislikes_count': dislikes_count
        })

    # Group by user_id and content_id, then apply the function
    engagement_aggregate = df[df['content_id'].isin(top_n_content)].groupby(['user_id', 'content_id']).apply(aggregate_engagement).reset_index()

    # Now, populate your user_vector_dict
    for _, row in engagement_aggregate.iterrows():
        user_id = row['user_id']
        content_id = row['content_id']
        idx = top_n_content.index(content_id)

        user_vector_dict[user_id]['millisecond_engaged_vector'][idx] = row['millisecond_engagement_sum']
        user_vector_dict[user_id]['like_vector'][idx] = row['likes_count']
        user_vector_dict[user_id]['dislike_vector'][idx] = row['dislikes_count']

    # Convert to DataFrame
    user_vector_df = pd.DataFrame.from_dict(user_vector_dict, orient='index')
    del user_vector_dict

    # Unpack vector columns into individual columns
    millisecond_columns = [f"ms_engaged_{i}" for i in range(TOP_CONTENT)]
    like_columns = [f"like_vector_{i}" for i in range(TOP_CONTENT)]
    dislike_columns = [f"dislike_vector_{i}" for i in range(TOP_CONTENT)]

    user_vector_df[millisecond_columns] = pd.DataFrame(user_vector_df['millisecond_engaged_vector'].tolist(), index=user_vector_df.index)
    user_vector_df[like_columns] = pd.DataFrame(user_vector_df['like_vector'].tolist(), index=user_vector_df.index)
    user_vector_df[dislike_columns] = pd.DataFrame(user_vector_df['dislike_vector'].tolist(), index=user_vector_df.index)

    # Drop the original vector columns
    user_vector_df.drop(['millisecond_engaged_vector', 'like_vector', 'dislike_vector'], axis=1, inplace=True)

    user_vector_df_new = user_vector_df
    user_vector_df_new = user_vector_df_new.merge(basic_like[['user_id','score']], right_on='user_id',left_index=True, how = "left").drop(columns=['user_id'])
    user_vector_df_new = user_vector_df_new.merge(basic_dislike[['user_id','score']], right_on='user_id',left_index=True, how = "left", suffixes=('_1', '_2')).drop(columns=['user_id'])
    user_vector_df_new = user_vector_df_new.merge(vigilante[['user_id','score']], right_on='user_id',left_index=True, how = "left").drop(columns=['user_id'])
    user_vector_df_new = user_vector_df_new.merge(indie[['user_id','score']], right_on='user_id',left_index=True, how = "left").drop(columns=['user_id'])
    user_vector_df_new.fillna(0, inplace=True)
    user_vector_df_new


    # Join User Vector To Df
    df = df.merge(
        user_vector_df.reset_index().rename(columns={'index': 'user_id'}),
        on='user_id'
    )
    del user_vector_df

    # Create overall features
    user_columns = (
        [f'ms_engaged_{i}' for i in range(TOP_CONTENT)] +
        [f'like_vector_{i}' for i in range(TOP_CONTENT)] +
        [f'dislike_vector_{i}' for i in range(TOP_CONTENT)]
    )
    user_features = df[user_columns]
    user_features_tensor = torch.FloatTensor(user_features.values)
    return df, user_features_tensor



df, user_features_tensor = df_to_user_tensor(df)
print(user_features_tensor.shape)

print(f'Len of df now is {len(df)}')

def generate_content_embeddings(df):
    content_features = df[content_columns]
    print(f'len of df is {len(df)}, len of user_features is {len(content_features)}')
    print(f'shape of content features is {len(content_features), len(content_features.columns)} while {len(content_columns)}')
    content_features_tensor = torch.FloatTensor(content_features.values)
    return content_features_tensor

content_features_tensor = generate_content_embeddings(df)
print(content_features_tensor.shape)

print(len(df[user_columns]))
print(len(df[content_columns]))

DISLIKE_ENGAGEMENT_TYPE_VALUE = 0
LIKE_ENGAGEMENT_TYPE_VALUE = 1
MS_ENGAGEMENT_TYPE_VALUE = 2


engagement_type_value = df[['engagement_type', 'engagement_value']]
engagement_type_value['engagement_type'] = engagement_type_value.apply(
    lambda x: (
       LIKE_ENGAGEMENT_TYPE_VALUE    if x['engagement_type'] == 'Like' and x['engagement_value'] ==  1 else # Like
       DISLIKE_ENGAGEMENT_TYPE_VALUE    if x['engagement_type'] == 'Like' and x['engagement_value'] == -1 else # Dislike
       MS_ENGAGEMENT_TYPE_VALUE                                                                          # MsEngagementType
    ), axis=1
)
engagement_type_tensor = torch.FloatTensor(engagement_type_value[['engagement_type']].values)
engagement_value_tensor = torch.FloatTensor(engagement_type_value[['engagement_value']].values)

engagement_type_value



print(user_features_tensor.shape)
print(content_features_tensor.shape)

"""# Training"""

import torch
import torch.nn as nn

# Ideally define this in your two_tower.py file and import
class TwoTowerModel(nn.Module):
    def __init__(self, user_dim, content_dim, output_dim):
        super(TwoTowerModel, self).__init__()
        self.user_tower = nn.Linear(user_dim, output_dim)
        self.content_tower = nn.Linear(content_dim, output_dim)

    def forward(self, user_features, content_features):
        user_embedding = self.user_tower(user_features)
        content_embedding = self.content_tower(content_features)
        return user_embedding, content_embedding

# Specify the input and output dimensions
user_dim = len(user_columns)
content_dim = len(content_columns)
output_dim = 64

# Create the model
model = TwoTowerModel(user_dim, content_dim, output_dim)

# Define the loss
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributions as D

class ContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0, lambda_reg=0.01, lambda_orthog=0.01):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin
        self.lambda_reg = lambda_reg
        self.lambda_orthog = lambda_orthog

    def calculate_targets(self, engagement_type_vector, engagement_value_vector):
        # Conditions for 0=dislike, 1=like, and 2=milliseconds engaged
        return torch.where(
              engagement_type_vector == DISLIKE_ENGAGEMENT_TYPE_VALUE,
              torch.zeros_like(engagement_type_vector), # dislike
              torch.where(
                  engagement_type_vector == LIKE_ENGAGEMENT_TYPE_VALUE,
                  torch.ones_like(engagement_type_vector), # like
                  torch.where(
                      engagement_value_vector < 500,
                      torch.zeros_like(engagement_type_vector), # bad engagement
                      torch.where(
                        engagement_value_vector <= 2500,
                        torch.ones_like(engagement_type_vector), # engaged 500ms => 2.5s
                        torch.zeros_like(engagement_type_vector), # bad engagement
                      )
                  ) # millisecond engaged with
              )
        )

    def forward(self, user_embedding, content_embedding, targets, with_debug=False):
        noise_factor = 0.0005
        user_embedding += noise_factor * torch.randn(*user_embedding.shape)
        content_embedding += noise_factor * torch.randn(*content_embedding.shape)

        cosine_sim = F.cosine_similarity(user_embedding, content_embedding, dim=1)

        # Contrastive loss
        loss_contrastive = torch.mean(
            (1 - targets) * torch.pow(cosine_sim, 2) +
            (targets)     * torch.pow(
                torch.clamp(self.margin - cosine_sim, min=0.0),
                2
            )
        )

        # Regularization terms
        reg_user = torch.norm(user_embedding, p=2)
        reg_content = torch.norm(content_embedding, p=2)
        regularization_loss = (reg_user + reg_content)

        # orthognal loss of content
        ortho_reg = torch.norm(
            torch.mm(content_embedding, content_embedding.t()) -
            torch.eye(content_embedding.size(0))
        )

        total_loss = (
            loss_contrastive +
            self.lambda_reg * regularization_loss +
            self.lambda_orthog * ortho_reg
        )
        if with_debug:
          print(f"""losses are:
              {loss_contrastive},
              {self.lambda_reg * regularization_loss},
              {self.lambda_orthog * ortho_reg},
          """)

        return total_loss


# Some Plotting
def random_cosine_similarity(content_embedding, n=10):
  similarity = torch.nn.functional.cosine_similarity(content_embedding.unsqueeze(0), content_embedding.unsqueeze(1), dim=2)
  # Ensure that the diagonal contains -inf so it won't interfere with the upper triangular part
  similarity = similarity - 2*torch.eye(similarity.shape[0])  # Subtracting 2 will give -2 for the diagonal, which is smaller than any possible cosine similarity (-1 to 1).

  # Get the upper triangular part without the diagonal
  indices = torch.triu_indices(row=similarity.shape[0], col=similarity.shape[1], offset=1)
  upper_triangular_part = similarity[indices[0], indices[1]]
  return upper_triangular_part[torch.randperm(upper_triangular_part.size(0))[:n]]


def plot_bins(content_embedding):
  similarity = torch.nn.functional.cosine_similarity(content_embedding.unsqueeze(0), content_embedding.unsqueeze(1), dim=2)
  # Ensure that the diagonal contains -inf so it won't interfere with the upper triangular part
  similarity = similarity - 2*torch.eye(similarity.shape[0])  # Subtracting 2 will give -2 for the diagonal, which is smaller than any possible cosine similarity (-1 to 1).

  # Get the upper triangular part without the diagonal
  indices = torch.triu_indices(row=similarity.shape[0], col=similarity.shape[1], offset=1)
  upper_triangular_part = similarity[indices[0], indices[1]]
  bins = 100
  x = range(-100, 100, 2)
  actual_hist = torch.histc(upper_triangular_part, bins=100, min=-1, max=1)
  actual_hist /= actual_hist.sum() # normalized
  plt.bar(x, actual_hist, align='center')
  plt.xlabel('Bins')
  plt.show()

from torch.utils.data import Dataset, DataLoader

class EngagementDataset(Dataset):
    def __init__(self, user_features, content_features, targets):
        self.user_features = user_features
        self.content_features = content_features
        self.targets = targets

    def __len__(self):
        return len(self.user_features)

    def __getitem__(self, index):
        return self.user_features[index], self.content_features[index], self.targets[index]

batch_size = 512

loss_function = ContrastiveLoss(
    margin=1.0, # Maybe normalize this by dimensions/batch_size?
    lambda_reg=0.01,  # Maybe normalize this by dimensions/batch_size?
    lambda_orthog=0.001 # Maybe normalize this by dimensions/batch_size?
)

# Split the data into train and test sets
from sklearn.model_selection import train_test_split
import torch.optim as optim
from torch.optim.lr_scheduler import CyclicLR

targets = loss_function.calculate_targets(engagement_type_tensor, engagement_value_tensor)
engagement_data = EngagementDataset(user_features_tensor, content_features_tensor, targets)

train_data, test_data = train_test_split(engagement_data, test_size=0.2, random_state=28)

# Further split the training data into train and validation sets
train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=28)

# Create DataLoader instances
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)


data_loader = DataLoader(engagement_data, batch_size=batch_size, shuffle=True)

from torch.optim.lr_scheduler import StepLR

max_lr = 0.005   # upper bound
optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)

# Scheduler - One Cycle Learning Rate Policy
scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,
                                               max_lr=max_lr,
                                               steps_per_epoch=len(train_loader),  # Assuming train_loader is your DataLoader
                                               epochs=50,  # The total number of epochs you're planning to train
                                               anneal_strategy='linear',
                                               pct_start=0.3,  # The fraction of epochs increasing LR, adjust based on your needs
                                               div_factor=25.0,  # max_lr divided by this factor gives the starting LR
                                               final_div_factor=10000.0)  # max_lr divided by this factor gives the ending LR

# Training loop
from tqdm import tqdm
import matplotlib.pyplot as plt
for epoch in range(40):
    model.train()
    for batch in tqdm(train_loader, desc=f'Training {epoch + 1}: '):
        user_features_batch, content_features_batch, targets_batch = batch
        optimizer.zero_grad()
        user_embedding, content_embedding = model(user_features_batch, content_features_batch)
        loss = loss_function(user_embedding, content_embedding, targets_batch)
        loss.backward()
        optimizer.step()
        # scheduler.step() -- depending on your scheduler
    loss = loss_function(user_embedding, content_embedding, targets_batch, with_debug=True)
    print(f'Epoch {epoch+1}, Last Batch Single Loss: {loss.item()}')

    # Validation
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for batch in tqdm(val_loader, desc=f'Validation {epoch + 1}: '):
            user_features_batch, content_features_batch, targets_batch = batch
            user_embedding, content_embedding = model(user_features_batch, content_features_batch)
            loss = loss_function(user_embedding, content_embedding, targets_batch)
            val_loss += loss.item()

        val_loss /= len(val_loader)
        print(f'Validation - Epoch {epoch+1}, Loss: {val_loss}')
    model.train()
    # scheduler.step(val_loss)
    print(f'Current Learning Rate: {optimizer.param_groups[0]["lr"]}')

    plot_bins(content_embedding)

    # Second validation
    print(f'Epoch {epoch+1}, Avg Cosine-Sims: {random_cosine_similarity(content_embedding, n=10)}')

torch.save(model.state_dict(),'drive/MyDrive/E4579 Fall 2023/clip_model.pth')

"""## Testing - kNN"""

mostfreq_10_eff.loc[81576]

model = TwoTowerModel(user_dim, content_dim, output_dim)
checkpoint = torch.load('drive/MyDrive/E4579 Fall 2023/clip_based_model.pth')
model.load_state_dict(checkpoint)

engnum_to_content_id = df['content_id'].to_dict()
content_id_to_engnum = {v:k for k,v in engnum_to_content_id.items()}

engnum_to_user_id = df['user_id'].to_dict()
user_id_to_engnum = {v:k for k,v in engnum_to_user_id.items()}

content_id_to_emb = lambda content_id: torch.tensor(
    df.iloc[content_id_to_engnum[content_id]][content_columns].to_numpy().astype(np.float32)
)

user_id_to_emb = lambda user_id: torch.tensor(
    df.iloc[user_id_to_engnum[user_id]][user_columns].to_numpy().astype(np.float32)
)

print(
    sorted(list(set(df.user_id)))
)
print(
    sorted(list(set(df.content_id)))
)

import numpy.linalg as npla
cossim = lambda u,v: u@v/(npla.norm(u)*npla.norm(v))

import torch.nn.functional as F
def content_id_to_cossim(my_cont_id):
  clip_pure = clip_embedding.loc[my_cont_id].vecs
  cossimdict = {}
  clipcos = []
  with torch.no_grad():
    for cont_id in sorted(list(set(df.content_id))):
      u,c1 = model(user_id_to_emb(1),
                  content_id_to_emb(cont_id))
      u,c2 = model(user_id_to_emb(1),
                   content_id_to_emb(my_cont_id))
      cossimdict[cont_id] = F.cosine_similarity(c1,c2,dim=0).item()
      clipcos.append(cossim(clip_pure, clip_embedding.loc[cont_id].vecs))


  dff = pd.DataFrame(list(cossimdict.items()),
                      columns=['content_id','cossim'])
  dff['clip_cos'] = clipcos
  return dff.sort_values('cossim', ascending=False)


def user_id_to_cossim(user_id):
  cossimdict = {}
  with torch.no_grad():
    for cont_id in sorted(list(set(df.content_id))):
      u,c = model(user_id_to_emb(user_id),
                  content_id_to_emb(cont_id))
      cossimdict[cont_id] = F.cosine_similarity(u,c,dim=0).item()
  return pd.DataFrame(list(cossimdict.items()),
                      columns=['content_id','cossim']).sort_values('cossim', ascending=False)

cossimdf_1 = content_id_to_cossim(28619)
cossimdf_1

plt.plot(list(cossimdf_1.cossim))
plt.plot(list(cossimdf_1.clip_cos))

def downloader(ax,im_id):
    row = s3url[s3url.id==im_id]
    bucket = row.s3_bucket.iloc[0]
    s3_id = row.s3_id.iloc[0]
    url = f'https://s3.amazonaws.com/{bucket}/{s3_id}'
    response = requests.get(url)

    #try:
    #    prompt = df[df.content_id==im_id].prompt.iloc[0]
    #except:
    #    prompt = 'Never shown'

    if response.status_code == 200:
        # Open the image using Pillow
        image = Image.open(io.BytesIO(response.content))

        # Display the image using matplotlib
        ax.imshow(image)
        ax.axis('off')  # Hide axis
        ax.set_title(mostfreq_10_eff.loc[im_id].iloc[0])
    else:
        print(f"Failed to retrieve the image. HTTP Status Code: {response.status_code}")

import matplotlib.pyplot as plt
import requests
from PIL import Image
import io

fig,ax = plt.subplots(2,1)
for i in range(2):
    downloader(ax[i], 28619)

fig,ax = plt.subplots(1,5,figsize=(10,10))

for i in range(5):
    row = cossimdf_1.iloc[i]
    content_id = row.content_id
    cossim = row.cossim
    print(cossim)
    downloader(ax[i], content_id)

content_to_group[content_to_group.group=='human_prompts']

cossimdf_81576 = content_id_to_cossim(81576)
fig,ax = plt.subplots(1,5,figsize=(10,10))

for i in range(5):
    row = cossimdf_81576.iloc[i]
    content_id = row.content_id
    cossim = row.cossim
    print(cossim)
    downloader(ax[i], content_id)







"""# Testing - production mode"""

tt = TwoTower()

data = tt.generate_content_embeddings(df)
index = mrpt.MRPTIndex(data)
index.build_autotune_sample(0.9, 10)

def get_ANN_recommednations(embedding, team, K):
    try:
        global index_to_content_id, INDEXES
        similar_indices, scores = INDEXES[team].query(embedding, k=K, return_distances=True)
        similar_content_ids = [index_to_content_id[idx] for idx in similar_indices]
        return similar_content_ids, scores
    except Exception as e:
        print(f"Error during ANN recommendations: {e}")
        return []

def get_ANN_recommendations_from_user(user_id, team, K):
    try:
        # Fetch engagements for user
        user_engagements = fetch_data_stub().filter(
            Engagement.user_id == user_id
        ).all()

        user_df = pd.DataFrame(user_engagements)

        user_embedding = team_wrappers[team].generate_user_embeddings(user_df)

        if len(user_embedding) == 0:
            return []

        return get_ANN_recommednations(user_embedding, team, K)
    except Exception as e:
        print(f"Error during ANN recommendations: {e}")
        return []





model_state_dict = model.state_dict()
torch.save(model_state_dict, 'drive/MyDrive/E4579 Fall 2023/clip_based_model.pth')

